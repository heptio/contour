# Copyright Project Contour Authors
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License.  You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
# License for the specific language governing permissions and limitations
# under the License.

# This check depends on the `--watch=endpoints` argument being given
# to integration-tester.

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-conformance-echo
$apply:
  fixture:
    as: echo

---

apiVersion: v1
kind: Service
metadata:
  name: ingress-conformance-echo
$apply:
  fixture:
    as: echo

---

# Test Case 1: non-TLS virtual hosts

# Create the HTTPProxy without rate limits first
# and wait until we get a 200 from it before applying
# rate limits and counting responses. This ensures
# the pods are up and receiving traffic and prevents
# the test from being flaky.

apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  name: vhostratelimit
spec:
  virtualhost:
    fqdn: vhostratelimit.projectcontour.io
  routes:
  - services:
    - name: echo
      port: 80
---

# Wait until we get a 200 from the proxy confirming
# the pods are up and serving traffic.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/"),
  "headers": {
    "Host": "vhostratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---


# Add a global rate limit policy on the virtual host.

apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  name: vhostratelimit
spec:
  virtualhost:
    fqdn: vhostratelimit.projectcontour.io
    rateLimitPolicy:
      global:
        descriptors:
          - entries:
              - genericKey:
                  value: vhostlimit
  routes:
  - services:
    - name: echo
      port: 80
---

# Make a request against the proxy, confirm a 200 response
# is returned since we're allowed one request per hour.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/"),
  "headers": {
    "Host": "vhostratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---

# Make another request against the proxy, confirm a 429
# response is now gotten since we've exceeded the rate
# limit.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/"),
  "headers": {
    "Host": "vhostratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 429)
}

---

# This proxy has a global rate limit on a route.
apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  name: routeratelimit
spec:
  virtualhost:
    fqdn: routeratelimit.projectcontour.io
  routes:
  - services:
    - name: echo
      port: 80
    rateLimitPolicy:
      global:
        descriptors:
          - entries:
              - genericKey:
                  value: routelimit
  - conditions:
      - prefix: /unlimited
    services:
    - name: echo
      port: 80
---

# Make a request against the proxy, confirm a 200 response
# is returned since we're allowed one request per hour.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/"),
  "headers": {
    "Host": "routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---

# Make another request against the proxy, confirm a 429
# response is now gotten since we've exceeded the rate
# limit.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/"),
  "headers": {
    "Host": "routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 429)
}

---

# Make a request against the route that doesn't have
# rate limiting to confirm we still get a 200 for that
# route.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.http("/unlimited"),
  "headers": {
    "Host": "routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---

# Test Case 2: TLS virtual hosts

# Create a self-signed issuer to give us secrets.

apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned
spec:
  selfSigned: {}

---

apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: tlsvhostratelimit
spec:
  dnsNames:
  - tls.vhostratelimit.projectcontour.io
  secretName: tlsvhostratelimit
  issuerRef:
    name: selfsigned
    kind: ClusterIssuer

---

# TLS vhost with a global rate limit policy on the virtual host.

apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  name: tlsvhostratelimit
spec:
  virtualhost:
    fqdn: tls.vhostratelimit.projectcontour.io
    tls:
      secretName: tlsvhostratelimit
    rateLimitPolicy:
      global:
        descriptors:
          - entries:
              - genericKey:
                  value: tlsvhostlimit
  routes:
  - services:
    - name: echo
      port: 80
---

# Make a request against the proxy, confirm a 200 response
# is returned since we're allowed one request per hour.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.https("/"),
  "headers": {
    "Host": "tls.vhostratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
  "tls_insecure_skip_verify": true,
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---

# Make another request against the proxy, confirm a 429
# response is now gotten since we've exceeded the rate
# limit.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.https("/"),
  "headers": {
    "Host": "tls.vhostratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
  "tls_insecure_skip_verify": true,
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 429)
}

---

apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: tlsrouteratelimit
spec:
  dnsNames:
  - tls.routeratelimit.projectcontour.io
  secretName: tlsrouteratelimit
  issuerRef:
    name: selfsigned
    kind: ClusterIssuer

---

# TLS vhost with a global rate limit policy on the route.

apiVersion: projectcontour.io/v1
kind: HTTPProxy
metadata:
  name: tlsrouteratelimit
spec:
  virtualhost:
    fqdn: tls.routeratelimit.projectcontour.io
    tls:
      secretName: tlsrouteratelimit
  routes:
  - services:
    - name: echo
      port: 80
    rateLimitPolicy:
      global:
        descriptors:
          - entries:
              - genericKey:
                  value: tlsroutelimit
  - conditions:
      - prefix: /unlimited
    services:
    - name: echo
      port: 80
---

# Make a request against the proxy, confirm a 200 response
# is returned since we're allowed one request per hour.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.https("/"),
  "headers": {
    "Host": "tls.routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
  "tls_insecure_skip_verify": true,
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}

---

# Make another request against the proxy, confirm a 429
# response is now gotten since we've exceeded the rate
# limit.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.https("/"),
  "headers": {
    "Host": "tls.routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
  "tls_insecure_skip_verify": true,
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 429)
}

---

# Make a request against the route that doesn't have
# rate limiting to confirm we still get a 200 for that
# route.

import data.contour.http.client
import data.contour.http.client.url
import data.contour.http.expect

Response := client.Get({
  "url": url.https("/unlimited"),
  "headers": {
    "Host": "tls.routeratelimit.projectcontour.io",
    "User-Agent": client.ua("global-rate-limit"),
  },
  "tls_insecure_skip_verify": true,
})

check_for_status_code [msg] {
  msg := expect.response_status_is(Response, 200)
}
